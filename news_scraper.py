# news_scraper.py
import aiohttp
import asyncio
import requests  # æ·»åŠ  requests åº“å¯¼å…¥
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from datetime import datetime, timedelta
import logging
from utils import async_timeout

logger = logging.getLogger(__name__)

# Binanceæ–°é—»æŠ“å–ç¤ºä¾‹
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_binance_news():
    url = "https://www.binance.com/en/support/announcement/c-48"
    news_list = []  # å°† news_list ç§»åˆ°å‡½æ•°å¼€å§‹å¤„
    skipped_count = 0  # æ–°å¢ï¼šè®°å½•è·³è¿‡çš„æ–°é—»æ•°é‡
    async with async_playwright() as p:
        # åœ¨fetch_binance_newsç­‰å‡½æ•°ä¸­ä¿®æ”¹æµè§ˆå™¨å¯åŠ¨é…ç½®
        browser = await p.chromium.launch(
            headless=True,
            # executable_path=os.environ.get('PLAYWRIGHT_CHROMIUM_PATH', ''),  # å…è®¸è‡ªå®šä¹‰è·¯å¾„
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",  # å‡å°‘å†…å­˜ä½¿ç”¨
                "--disable-gpu",            # ç¦ç”¨GPUåŠ é€Ÿ
                "--single-process"          # ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
            ]
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        page = await context.new_page()
        
        try:
            print("æ­£åœ¨åŠ è½½ Binance é¡µé¢...")
            
            # è®¾ç½®è¯·æ±‚æ‹¦æˆª
            await page.route("**/*", lambda route: route.continue_())
            
            # å¢åŠ åŠ è½½è¶…æ—¶æ—¶é—´ï¼Œç­‰å¾…éªŒè¯ç åŠ è½½
            await page.goto(url, wait_until="networkidle", timeout=120000)
            print("é¡µé¢åˆæ­¥åŠ è½½å®Œæˆ")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await page.wait_for_timeout(5000)
            
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            selector = "div.bn-flex.flex-col.py-6"
            print(f"ç­‰å¾…é€‰æ‹©å™¨: {selector}")
            await page.wait_for_selector(selector, timeout=60000)
            
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            print("ğŸ” å¼€å§‹æŠ“å– Binance çš„æ–°é—»...")
            print(html[:100])  # åªæ‰“å°HTMLçš„å‰1000ä¸ªå­—ç¬¦

            # æ›´æ–°é€‰æ‹©å™¨
            news_items = soup.select("div.bn-flex a.text-PrimaryText")
            print(f"æ‰¾åˆ° {len(news_items)} ä¸ªæ–°é—»é¡¹")
            
            # news_items = soup.select("div.items-center")
            # news_items = soup.select("div.flex-col.items-center a.text-primaryText")
            # news_items = soup.select("div.bn-flex a.text-primaryText")
            # news_list = []
            
            for item in news_items:
                title = None  # åˆå§‹åŒ– title ä¸ºç©º
                # h3_items = item.find_all("h3", class_=["typography-body1-1", "typography-body3"])
                h3_items = item.find("h3", class_="typography-body1-1")
                title = h3_items.text.strip() if h3_items else None
                # title = h3_items[0].text.strip() if h3_items else None
                # if h3_items:
                #      title = h3_items[0].text.strip()
                
                link = "https://www.binance.com" + item["href"] if item.get("href") else None
            
                # è·å–æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨æä¾›çš„é€‰æ‹©å™¨
                time_tag = item.find_next("div", class_="typography-caption1")  # æŸ¥æ‰¾ç´§é‚»çš„æ—¶é—´å…ƒç´ 
                time = time_tag.text.strip() if time_tag else "No date found"
            
                 # åªåœ¨é“¾æ¥å­˜åœ¨æ—¶æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨
                if title and link and time:
                    news_list.append({
                        "title": title, 
                        "link": link, 
                        "time": format_news_time(time),
                        "source": "Binance"
                    })
                else:
                    # print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–°é—»é¡¹: {item}")
                    skipped_count += 1  # åªå¢åŠ è®¡æ•°ï¼Œä¸æ‰“å°è¯¦ç»†ä¿¡æ¯

            # å…³é—­æµè§ˆå™¨
            await browser.close()
        except Exception as e:
            print(f"Binance æŠ“å–å‡ºé”™: {e}")
            print(f"\né”™è¯¯è¯¦ç»†ä¿¡æ¯:")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        finally:
            await browser.close()
        
    # å°†ç»Ÿè®¡ä¿¡æ¯ç§»åˆ° async with å—å¤–é¢ï¼Œä¸å‡½æ•°çš„ç¼©è¿›çº§åˆ«ç›¸åŒ
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)
    
    print("\n=== æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»ï¼Œè·³è¿‡ {skipped_count} æ¡æ— æ•ˆé¡¹\n")
    
    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")
    
    return list(unique_news)    


# ä½ å¯ä»¥ä¸ºå…¶å®ƒäº¤æ˜“æ‰€ç¼–å†™ç±»ä¼¼çš„æŠ“å–å‡½æ•°
# OKXæ–°é—»æŠ“å–ç¤ºä¾‹
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_okx_news():
    url = "https://www.okx.com/help/section/announcements-new-listings"
    news_list = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            print("æ­£åœ¨åŠ è½½ OKX é¡µé¢...")
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
            await page.goto(url, wait_until="domcontentloaded", timeout=120000)
            await page.wait_for_load_state("networkidle", timeout=60000)
            await page.wait_for_selector("li.index_articleItem__d-8iK", timeout=60000)
            
            # è·å–é¡µé¢ HTML
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            print("ğŸ” å¼€å§‹æŠ“å– OKX çš„æ–°é—»...")
            print(html[:100])  # åªæ‰“å°HTMLçš„å‰1000ä¸ªå­—ç¬¦
            # è·å–æ‰€æœ‰æ–°é—»é¡¹
            news_items = soup.select("li.index_articleItem__d-8iK")
            print(f"æŠ“å–åˆ° {len(news_items)} æ¡æ–°é—»")
            news_list = []
            
            for item in news_items:
                title = None  # åˆå§‹åŒ– title ä¸ºç©º
                title_tag = item.find("div", class_="index_title__iTmos")
                title = title_tag.text.strip() if title_tag else None

                link_tag = item.find("a", href=True)
                link = link_tag["href"] if link_tag else None

                # è·å–æ—¶é—´ä¿¡æ¯
                time_tag = item.find("span", {"data-testid": "DateDisplay"})  # æ ¹æ® data-testid å±æ€§è·å–æ—¶é—´
                time = time_tag.text.strip() if time_tag else "No date found"

                # æ‰“å°æ–°é—»çš„æ—¶é—´æ ¼å¼
                # print(f"æŠ“å–åˆ°çš„æ—¶é—´: {time}")
                
                # åªåœ¨é“¾æ¥å­˜åœ¨æ—¶æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨
                if title and link and time:
                    full_link = "https://www.okx.com" + link if not link.startswith("http") else link
                    # news_list.append({"title": title, "link": full_link, "time": time})
                    news_list.append({
                          "title": title, 
                          "link": full_link, 
                          "time": format_news_time(time),
                          "source": "OKX"
                        })
                else:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–°é—»é¡¹: {item}")

            await browser.close()

        except Exception as e:
            print(f"OKX æŠ“å–å‡ºé”™: {e}")
        finally:
            await browser.close()

    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)

    print("\n=== æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")

    return list(unique_news)

# Bitgetæ–°é—»æŠ“å–
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_bitget_news():
    url = "https://www.bitget.com/support/categories/11865590960081"

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()

        # å°è¯•è·³è¿‡é˜²çˆ¬è™«æ£€æŸ¥
        await page.goto(url, wait_until="domcontentloaded")

        # ç­‰å¾… Spot å’Œ Futures åŒºåŸŸåŠ è½½
        try:
            await page.wait_for_selector("section.ArticleList_item_pair__vmMrx", timeout=90000)  # å¢åŠ è¶…æ—¶
        except Exception as e:
            print(f"é”™è¯¯ï¼š{e}")
            html = await page.content()
            print(html[:1000])  # è¾“å‡ºHTMLçš„å‰1000ä¸ªå­—ç¬¦ä»¥å¸®åŠ©è°ƒè¯•

        # è·å–é¡µé¢ HTML
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        # æ‰“å°éƒ¨åˆ† HTML å†…å®¹æ¥è°ƒè¯•
        print("ğŸ” å¼€å§‹æŠ“å– Bitget çš„æ–°é—»...")
        print(html[:100])  # åªæ‰“å°HTMLçš„å‰1000ä¸ªå­—ç¬¦

        # è·å– Spot å’Œ Futures ä¸Šå¸ä¿¡æ¯
        spot_items = soup.select('section.ArticleList_item_pair__vmMrx')
        news_list = []

        # å¤„ç† Spot ç±»å‹çš„æ–°é—»
        for item in spot_items:
            title = None
            link = None
            time = "No date found"
            
            # è·å–æ ‡é¢˜å’Œé“¾æ¥
            title_tag = item.find("span", class_="ArticleList_item_title__u3fLL")
            if title_tag:
                title = title_tag.get_text(strip=True)
                link_tag = title_tag.find("a", href=True)
                if link_tag:
                    link = "https://www.bitget.com" + link_tag["href"] if not link_tag["href"].startswith("http") else link_tag["href"]
            
            # è·å–æ—¶é—´
            time_tag = item.find("div", class_="ArticleList_item_date__nEqio")
            if time_tag:
                time = time_tag.get_text(strip=True)

            # åªåœ¨æœ‰æœ‰æ•ˆæ ‡é¢˜ã€é“¾æ¥å’Œæ—¶é—´æ—¶æ·»åŠ åˆ°åˆ—è¡¨
            if title and link:
                # news_list.append({"title": title, "link": link, "time": time})
                news_list.append({
                      "title": title, 
                      "link": link, 
                      "time": format_news_time(time),
                      "source": "Bitget"
                    })

        await browser.close()

    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)

    # è¾“å‡ºç»“æœ
    print("\n=== æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")

    return list(unique_news)


# # Bybitæ–°é—»æŠ“å– - æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨ç½‘é¡µæŠ“å–ï¼Œç°å·²æ³¨é‡Šæ‰
# å¤‡ä»½ï¼ŒBybitæ–°é—»æŠ“å– headless=Falseæ—¶OKï¼Œheadless=Trueæ—¶æŠ¥é”™ 
# async def fetch_bybit_news():
#     url = "https://announcements.bybit.com/?category=new_crypto&page=1"
#     news_list = []
#     async with async_playwright() as p:
#         # browser = await p.chromium.launch(headless=False)
#         browser = await p.chromium.launch(
#             headless=False,
#             args=[      
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox",
#                 "--disable-http2",
#                 "--disable-cache",
#                 "--disable-web-security",
#                 "--disable-gl",
#                 "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
#             ]
#         )
#         page = await browser.new_page()

#         # # å°è¯•è·³è¿‡é˜²çˆ¬è™«æ£€æŸ¥
#         try:
#             print("æ­£åœ¨åŠ è½½é¡µé¢...")
#             await page.goto(url, wait_until="domcontentloaded", timeout=240000)
#             await page.wait_for_load_state("networkidle", timeout=60000)  # ç­‰å¾…ç½‘ç»œé—²ç½®
#             # await page.goto(url, wait_until="networkidle", timeout=240000)  # æ›´é•¿çš„è¶…æ—¶
#             # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½åå†è¿›è¡Œåç»­æ“ä½œ
#             await page.wait_for_selector("div.article-list", timeout=60000)  # ç­‰å¾…å…ƒç´ åŠ è½½
#             print("é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹æŠ“å–å†…å®¹...")
#         except Exception as e:
#             print(f"åŠ è½½é¡µé¢æ—¶å‡ºç°é”™è¯¯: {e}")
#             html = await page.content()
#             print("é¡µé¢HTMLå‰1000ä¸ªå­—ç¬¦:")
#             print(html[:1000])  # è¾“å‡ºHTMLçš„å‰1000ä¸ªå­—ç¬¦ä»¥å¸®åŠ©è°ƒè¯•


#         # è·å–é¡µé¢ HTML
#         html = await page.content()
#         soup = BeautifulSoup(html, "html.parser")
#         # æ‰“å°éƒ¨åˆ† HTML å†…å®¹æ¥è°ƒè¯•
#         print("ğŸ” å¼€å§‹æŠ“å– Bybit çš„æ–°é—»...")
#         print(html[:100])  # åªæ‰“å°HTMLçš„å‰1000ä¸ªå­—ç¬¦
        
#         # è·å–æ‰€æœ‰æ–°é—»é¡¹
#         news_items = soup.select("div.article-list a")
#         news_list = []
        
#         for item in news_items:
#             title = None  # åˆå§‹åŒ– title ä¸ºç©º
#             link = item["href"] if item.get("href") else None
            
#             # è·å–æ ‡é¢˜
#             title_tag = item.find("span")
#             title = title_tag.text.strip() if title_tag else None
            
#             # è·å–æ—¶é—´
#             time_tag = item.find("div", class_="article-item-date")
#             time = time_tag.text.strip() if time_tag else "No date found"
            
#             # åªåœ¨é“¾æ¥å­˜åœ¨æ—¶æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨
#             if title and link:
#                 full_link = "https://announcements.bybit.com" + link if not link.startswith("http") else link
#                 # news_list.append({"title": title, "link": full_link, "time": time})
#                 news_list.append({
#                       "title": title, 
#                       "link": full_link, 
#                       "time": format_news_time(time),
#                       "source": "Bybit"
#                     })

#         await browser.close()

#     # ç»Ÿè®¡ä¿¡æ¯
#     total_count = len(news_list)
#     unique_news = {item["title"]: item for item in news_list}.values()
#     filtered_count = len(unique_news)

#     print("\n=== æŠ“å–ç»Ÿè®¡ ===")
#     print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
#     print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

#     print("\n=== æŠ“å–ç»“æœ ===\n")
#     for item in unique_news:
#         print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")

#     return list(unique_news)

# ä½¿ç”¨ Bybit å®˜æ–¹ API è·å–æ–°é—»
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_bybit_news():
    print("å¼€å§‹é€šè¿‡ Bybit å®˜æ–¹ API æŠ“å–æ–°é—»...")
    news_list = []
    
    # å¯¼å…¥ requests åº“
    import requests
    
    # Bybit å…¬å‘Š API å®˜æ–¹ç«¯ç‚¹
    url = "https://api.bybit.com/v5/announcements/index"
    
    # è¯·æ±‚å‚æ•° (æ ¹æ®å®˜æ–¹æ–‡æ¡£)
    params = {
        "locale": "en-US",  # ä½¿ç”¨è‹±æ–‡ï¼Œæ›´ç¨³å®š
        "type": "new_crypto",  # æ–°å¸ä¸Šçº¿ç±»åˆ«
        "page": 1,
        "limit": 20  # è·å–æœ€æ–°çš„20æ¡å…¬å‘Š
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept": "application/json"
    }
    
    try:
        print(f"æ­£åœ¨è¯·æ±‚ Bybit API: {url}")
        
        # ä½¿ç”¨åŒæ­¥è¯·æ±‚ï¼Œè®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
        response = requests.get(url, params=params, headers=headers, timeout=60)
        
        print(f"API å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            
            # æ£€æŸ¥ API å“åº”ç»“æ„ (æ ¹æ®å®˜æ–¹æ–‡æ¡£)
            if data.get("retCode") == 0 and "result" in data:
                result = data["result"]
                announcements = result.get("list", [])
                print(f"è·å–åˆ° {len(announcements)} æ¡å…¬å‘Š")
                
                for item in announcements:
                    title = item.get("title")
                    url = item.get("url")
                    publish_time = item.get("publishTime")
                    description = item.get("description", "")
                    
                    if title:
                        # å¤„ç†æ—¶é—´
                        formatted_time = None
                        if publish_time:
                            try:
                                # API è¿”å›çš„æ—¶é—´é€šå¸¸æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³
                                dt = datetime.fromtimestamp(int(publish_time) / 1000)
                                formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S UTC")
                            except Exception as e:
                                print(f"æ—¶é—´æ ¼å¼åŒ–é”™è¯¯: {e}, åŸå§‹æ—¶é—´: {publish_time}")
                                formatted_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
                        
                        news_list.append({
                            "title": title,
                            "link": url,
                            "time": formatted_time,
                            "description": description,
                            "source": "Bybit"
                        })
                        # print(f"âœ… æˆåŠŸæŠ“å–: {title} | æ—¶é—´: {formatted_time}")
            else:
                error_msg = data.get("retMsg", "æœªçŸ¥é”™è¯¯")
                print(f"API å“åº”é”™è¯¯: {error_msg}")
                print(f"å®Œæ•´å“åº”: {data}")
        else:
            print(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(response.text)
    except requests.exceptions.Timeout:
        print("API è¯·æ±‚è¶…æ—¶ï¼Œå°è¯•å¤‡ç”¨ç«¯ç‚¹...")
        # å°è¯•å¤‡ç”¨ç«¯ç‚¹
        backup_url = "https://api.bytick.com/v5/announcements/index"
        try:
            response = requests.get(backup_url, params=params, headers=headers, timeout=60)
            if response.status_code == 200:
                data = response.json()
                if data.get("retCode") == 0 and "result" in data:
                    result = data["result"]
                    announcements = result.get("list", [])
                    print(f"å¤‡ç”¨ API è·å–åˆ° {len(announcements)} æ¡å…¬å‘Š")
                    
                    for item in announcements:
                        title = item.get("title")
                        url = item.get("url")
                        publish_time = item.get("publishTime")
                        description = item.get("description", "")
                        
                        if title:
                            # å¤„ç†æ—¶é—´
                            formatted_time = None
                            if publish_time:
                                try:
                                    dt = datetime.fromtimestamp(int(publish_time) / 1000)
                                    formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S UTC")
                                except Exception as e:
                                    print(f"æ—¶é—´æ ¼å¼åŒ–é”™è¯¯: {e}, åŸå§‹æ—¶é—´: {publish_time}")
                                    formatted_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
                            
                            news_list.append({
                                "title": title,
                                "link": url,
                                "time": formatted_time,
                                "description": description,
                                "source": "Bybit"
                            })
                            print(f"âœ… æˆåŠŸæŠ“å–: {title} | æ—¶é—´: {formatted_time}")
                else:
                    print(f"å¤‡ç”¨ API å“åº”é”™è¯¯: {data.get('retMsg', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print(f"å¤‡ç”¨ API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            print(f"å¤‡ç”¨ API æŠ“å–å‡ºé”™: {e}")
    except Exception as e:
        print(f"Bybit API æŠ“å–å‡ºé”™: {e}")
        print(f"\né”™è¯¯è¯¦ç»†ä¿¡æ¯:")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)

    print("\n=== Bybit æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")

    return list(unique_news)

# ç»Ÿä¸€å¤„ç†æ–°é—»æ—¶é—´æ ¼å¼
def format_news_time(news_time):
    """
    ç»Ÿä¸€å¤„ç†æ–°é—»æ—¶é—´çš„æ ¼å¼
    æ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. '2025-02-27'
    2. 'Published on Feb 20, 2025'
    3. '2025-03-03 10:41'
    4. 'Feb 26, 2025'
    5. '03/12/2025, 03:12:02'
    6. '1 hours 6 min 16 sec ago'
    7. '2 days ago'
    8. '3 minutes ago'
    è¿”å›ç»Ÿä¸€æ ¼å¼çš„æ—¶é—´ï¼š'YYYY-MM-DD HH:MM:SS UTC'
    """
    # å¤„ç†ç›¸å¯¹æ—¶é—´æ ¼å¼
    try:
        if "ago" in news_time.lower():
            now = datetime.now()
            parts = news_time.lower().split()
            
            if "day" in parts[1]:
                days = int(parts[0])
                result_time = now - timedelta(days=days)
            elif "hour" in parts[1]:
                hours = int(parts[0])
                minutes = int(parts[2]) if len(parts) > 4 and "min" in parts[3] else 0
                result_time = now - timedelta(hours=hours, minutes=minutes)
            elif "minute" in parts[1] or "min" in parts[1]:
                minutes = int(parts[0])
                result_time = now - timedelta(minutes=minutes)
            elif "second" in parts[1] or "sec" in parts[1]:
                seconds = int(parts[0])
                result_time = now - timedelta(seconds=seconds)
            else:
                return now.strftime("%Y-%m-%d %H:%M:%S UTC")
                
            return result_time.strftime("%Y-%m-%d %H:%M:%S UTC")
    except (ValueError, IndexError):
        pass

    # ä¿ç•™åŸæœ‰çš„æ ¼å¼å¤„ç†ä»£ç 
    try:
        if len(news_time) == 10:  # æ ¼å¼ 2025-02-27
            return datetime.strptime(news_time, "%Y-%m-%d").strftime("%Y-%m-%d 00:00:00 UTC")
    except ValueError:
        pass

    # å¤„ç†æ ¼å¼2: Published on Feb 20, 2025
    try:
        if news_time.startswith("Published on"):
            news_time = news_time[13:].strip()  # å»æ‰ "Published on"
            return datetime.strptime(news_time, "%b %d, %Y").strftime("%Y-%m-%d 00:00:00 UTC")
    except ValueError:
        pass
    # å¤„ç†æ ¼å¼3: 2025-03-03 10:41
    try:
        if len(news_time) > 10:  # æ ¼å¼ 2025-03-03 10:41
            return datetime.strptime(news_time, "%Y-%m-%d %H:%M").strftime("%Y-%m-%d %H:%M:%S UTC")
    except ValueError:
        pass

    # å¤„ç†æ ¼å¼4: Feb 26, 2025
    try:
        if len(news_time.split()) == 3:  # æ ¼å¼ Feb 26, 2025
            return datetime.strptime(news_time, "%b %d, %Y").strftime("%Y-%m-%d 00:00:00 UTC")
    except ValueError:
        pass
    
    # å¤„ç†æ ¼å¼5: æ·»åŠ å¯¹ MM/DD/YYYY, HH:MM:SS æ ¼å¼çš„æ”¯æŒ
    try:
        if '/' in news_time and ',' in news_time:
            parsed_time = datetime.strptime(news_time.strip(), "%m/%d/%Y, %H:%M:%S")
            return parsed_time.strftime("%Y-%m-%d %H:%M:%S UTC")
    except ValueError:
        pass
    
    print(f"âŒ æ— æ³•è§£ææ—¶é—´: {news_time}")
    return None

    # å¤„ç†æ ¼å¼5: MM/DD/YYYY, HH:MM:SS æ ¼å¼çš„æ”¯æŒ
    # try:
    #     if '/' in news_time and ',' in news_time:
    #         date_part, time_part = news_time.split(',')
    #         parsed_time = datetime.strptime(news_time.strip(), "%m/%d/%Y, %H:%M:%S")
    #         return parsed_time.strftime("%Y-%m-%d %H:%M:%S UTC")
    # except ValueError:
    #     pass


# æµ‹è¯•æŠ“å–åŠŸèƒ½
# Coinbaseæ–°é—»æŠ“å–ï¼ŒCoinbaseé€šè¿‡Twitterå‘å¸ƒçš„æ¶ˆæ¯ï¼Œæ²¡æœ‰ç½‘é¡µï¼šhttps://x.com/CoinbaseAssetsï¼Œhttps://x.com/CoinbaseIntExch
# async def fetch_coinbase_news():
#     url = "https://www.coinbase.com/browse/announcements"
#     news_list = []

#     async with async_playwright() as p:
#         browser = await p.chromium.launch(headless=True)
#         page = await browser.new_page()
        
#         try:
#             print("æ­£åœ¨åŠ è½½ Coinbase é¡µé¢...")
#             await page.goto(url, wait_until="domcontentloaded", timeout=60000)
#             await page.wait_for_selector("article", timeout=60000)
            
#             html = await page.content()
#             soup = BeautifulSoup(html, "html.parser")
#             print("ğŸ” å¼€å§‹æŠ“å– Coinbase çš„æ–°é—»...")
            
#             news_items = soup.select("article")
            
#             for item in news_items:
#                 title_tag = item.find("h2")
#                 title = title_tag.text.strip() if title_tag else None
                
#                 link_tag = item.find("a", href=True)
#                 link = "https://www.coinbase.com" + link_tag["href"] if link_tag else None
                
#                 time_tag = item.find("time")
#                 time = time_tag["datetime"] if time_tag else "No date found"
                
#                 if title and link:
#                     news_list.append({
#                         "title": title,
#                         "link": link,
#                         "time": format_news_time(time),
#                         "source": "Coinbase"
#                     })
                    
#         except Exception as e:
#             print(f"Coinbase æŠ“å–å‡ºé”™: {e}")
#         finally:
#             await browser.close()

#     # ç»Ÿè®¡ä¿¡æ¯
#     total_count = len(news_list)
#     unique_news = {item["title"]: item for item in news_list}.values()
#     filtered_count = len(unique_news)
    
#     print("\n=== Coinbase æŠ“å–ç»Ÿè®¡ ===")
#     print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
#     print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")
    
#     return list(unique_news)




# KuCoinæ–°é—»æŠ“å–
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_kucoin_news():
    url = "https://www.kucoin.com/announcement/new-listings"
    news_list = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",  # å‡å°‘å†…å­˜ä½¿ç”¨
                "--disable-gpu",            # ç¦ç”¨GPUåŠ é€Ÿ
                "--single-process"          # ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
            ]
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            print("æ­£åœ¨åŠ è½½ KuCoin é¡µé¢...")
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_selector("ul.kux-e8uvvx", timeout=60000)
            
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            print("ğŸ” å¼€å§‹æŠ“å– KuCoin çš„æ–°é—»...")
            
            # è·å–æ‰€æœ‰æ–°é—»é¡¹
            news_items = soup.select("ul.kux-e8uvvx > li")
            
            for item in news_items:
                title = None  # åˆå§‹åŒ– title ä¸ºç©º
                link = None
                time = "No date found"
                
                # è·å–æ ‡é¢˜å’Œé“¾æ¥
                link_tag = item.find("a", href=True)
                if link_tag:
                    link = "https://www.kucoin.com" + link_tag["href"]
                    title_tag = link_tag.find("span")
                    title = title_tag.text.strip() if title_tag else None
                
                # è·å–æ—¶é—´
                time_tag = item.find("p", class_='kux-q65diy')
                time = time_tag.text.strip() if time_tag else "No date found"
                
                if title and link:
                    news_list.append({
                        "title": title,
                        "link": link,
                        "time": format_news_time(time),
                        "source": "KuCoin"
                    })
                    
        except Exception as e:
            print(f"KuCoin æŠ“å–å‡ºé”™: {e}")
            html = await page.content()
            print("é¡µé¢HTMLå‰100ä¸ªå­—ç¬¦:")
            print(html[:1000])
        finally:
            await browser.close()

    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)
    
    print("\n=== KuCoin æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")
    
    return list(unique_news)

# Gate.ioæ–°é—»æŠ“å–
@async_timeout(300)  # 5åˆ†é’Ÿè¶…æ—¶
async def fetch_gate_news():
    url = "https://www.gate.io/announcements/newlisted"
    news_list = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",  # å‡å°‘å†…å­˜ä½¿ç”¨
                "--disable-gpu",  # ç¦ç”¨GPUåŠ é€Ÿ
                "--single-process"          # ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
            ],
            timeout=120000  # å¢åŠ æµè§ˆå™¨å¯åŠ¨è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        page = await context.new_page()
        
        try:
            print("æ­£åœ¨åŠ è½½ Gate.io é¡µé¢...")
            # è®¾ç½®è¯·æ±‚æ‹¦æˆªï¼Œç±»ä¼¼äºBinanceçš„å¤„ç†æ–¹å¼
            await page.route("**/*", lambda route: route.continue_())
            
            # å¢åŠ é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
            await page.goto(url, wait_until="networkidle", timeout=120000)
            print("é¡µé¢åˆæ­¥åŠ è½½å®Œæˆ")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await page.wait_for_timeout(5000)
            
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            selector = "div.flex.flex-col.gap-6.sm\\:gap-8 a"
            print(f"ç­‰å¾…é€‰æ‹©å™¨: {selector}")
            await page.wait_for_selector(selector, timeout=60000)
            
            # è·å–é¡µé¢ HTML
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            print("ğŸ” å¼€å§‹æŠ“å– Gate.io çš„æ–°é—»...")
            print(f"é¡µé¢HTMLå‰100ä¸ªå­—ç¬¦: {html[:100]}")  # æ‰“å°é¡µé¢å¼€å¤´éƒ¨åˆ†
            # è·å–æ‰€æœ‰æ–°é—»é¡¹ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            news_items = soup.select("div.flex.flex-col.gap-6.sm\\:gap-8 a")
            # print(f"æ‰¾åˆ° {len(news_items)} ä¸ªæ–°é—»é¡¹")
            
            for item in news_items:
                title = None  # åˆå§‹åŒ– title ä¸ºç©º
                link = None
                time = "No date found"
                
                # è·å–æ ‡é¢˜
                title_tag = item.find("p", class_="font-medium text-subtitle line-clamp-2")
                title = title_tag.text.strip() if title_tag else None
                
                # è·å–é“¾æ¥
                link = "https://www.gate.io" + item["href"] if item.get("href") else None
                
                # è·å–æ—¶é—´ - ä½¿ç”¨å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„ç»“æ„
                time_div = item.find("div", class_="flex gap-5 text-body-s text-t3")
                if time_div:
                    time_inner_div = time_div.find("div", class_="flex items-center gap-1")
                    if time_inner_div:
                        time_span = time_inner_div.find("span")
                        time = time_span.text.strip() if time_span else "No date found"
                
                if title and link:
                    news_list.append({
                        "title": title,
                        "link": link,
                        "time": format_news_time(time),
                        "source": "Gate.io"
                    })
                    # print(f"âœ… æˆåŠŸæŠ“å–: {title} | æ—¶é—´: {time}")
                else:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–°é—»é¡¹: æ ‡é¢˜={title}, é“¾æ¥={link}")
                    
        except Exception as e:
            print(f"Gate.io æŠ“å–å‡ºé”™: {e}")
            print(f"\né”™è¯¯è¯¦ç»†ä¿¡æ¯:")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            html = await page.content()
            print("é¡µé¢HTMLå‰1000ä¸ªå­—ç¬¦:")
            print(html[:100])  # è¾“å‡ºHTMLå¸®åŠ©è°ƒè¯•
        finally:
            await browser.close()

    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(news_list)
    unique_news = {item["title"]: item for item in news_list}.values()
    filtered_count = len(unique_news)
    
    print("\n=== Gate.io æŠ“å–ç»Ÿè®¡ ===")
    print(f"ğŸ“Œ æ€»å…±æŠ“å– {total_count} æ¡æ–°é—»")
    print(f"ğŸ” å»é‡åå‰©ä½™ {filtered_count} æ¡æ–°é—»\n")

    # print("\n=== æŠ“å–ç»“æœ ===\n")
    # for item in unique_news:
    #     print(f"ğŸ“Œ {item['title']}\nğŸ”— {item['link']}\nğŸ“… {item['time']}\n")
    
    return list(unique_news)

# æ›´æ–° main å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•°ï¼šå¹¶è¡ŒæŠ“å–æ‰€æœ‰äº¤æ˜“æ‰€çš„æ–°é—»ï¼Œå¹¶åˆå¹¶ç»“æœ
    
    è®¾ç½®äº†æ€»ä½“è¶…æ—¶æ§åˆ¶ï¼Œç¡®ä¿å³ä½¿æŸä¸ªäº¤æ˜“æ‰€æŠ“å–å¤±è´¥ï¼Œä¹Ÿèƒ½è¿”å›å…¶ä»–äº¤æ˜“æ‰€çš„ç»“æœ
    
    Returns:
        list: åˆå¹¶åçš„æ–°é—»åˆ—è¡¨
    """
    logger.info("å¼€å§‹æŠ“å–æ‰€æœ‰äº¤æ˜“æ‰€æ–°é—»...")
    start_time = datetime.now()
    
    # æ£€æŸ¥ç¯å¢ƒ
    is_railway = os.environ.get('RAILWAY_ENVIRONMENT') is not None
    environment_name = "Railwayç¯å¢ƒ" if is_railway else "æœ¬åœ°ç¯å¢ƒ"
    
    logger.info(f"ğŸŒ å½“å‰åœ¨ã€{environment_name}ã€‘ä¸­æ‰§è¡ŒæŠ“å–ä»»åŠ¡")
    
    try:
        # å¦‚æœåœ¨ Railway ç¯å¢ƒä¸­ï¼Œç¡®ä¿ Playwright ä¾èµ–å·²å®‰è£…
        if is_railway:
            try:
                logger.info("åœ¨ Railway ç¯å¢ƒä¸­ï¼Œå°è¯•å®‰è£… Playwright ä¾èµ–")
                # å®‰è£… Playwright ä¾èµ–
                subprocess.run([sys.executable, "-m", "playwright", "install", "--with-deps", "chromium"], 
                               check=True, capture_output=True)
                logger.info("Playwright ä¾èµ–å®‰è£…æˆåŠŸ")
            except Exception as e:
                logger.error(f"å®‰è£… Playwright ä¾èµ–å¤±è´¥: {e}")
                logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
                # å³ä½¿å®‰è£…å¤±è´¥ï¼Œä»ç„¶å°è¯•ä½¿ç”¨ Playwright æ–¹æ³•
                logger.info("å°½ç®¡å®‰è£…ä¾èµ–å¤±è´¥ï¼Œä»å°†å°è¯•ä½¿ç”¨ Playwright æŠ“å–æ–¹æ³•")
        
        # ç»Ÿä¸€ä½¿ç”¨ Playwright æŠ“å–æ–¹æ³•
        logger.info("ä½¿ç”¨ Playwright æŠ“å–æ–¹æ³•")
        
        # é…ç½® Playwright å¯åŠ¨é€‰é¡¹ï¼Œé€‚åº”äº‘ç¯å¢ƒ
        browser_launch_options = {
            "headless": True,
            "args": [
                "--disable-gpu",
                "--disable-dev-shm-usage",
                "--disable-setuid-sandbox",
                "--no-sandbox",
                "--single-process",
                "--no-zygote"
            ]
        }

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [
            fetch_binance_news(),
            fetch_okx_news(),
            fetch_bitget_news(),
            fetch_bybit_news(),
            fetch_kucoin_news(),
            fetch_gate_news()
        ]

        # ä½¿ç”¨ gather å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œè®¾ç½® return_exceptions=True é¿å…ä¸€ä¸ªä»»åŠ¡å¤±è´¥å½±å“å…¶ä»–ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        all_news = []
        exchange_names = ["Binance", "OKX", "Bitget", "Bybit", "KuCoin", "Gate.io"]
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # å¦‚æœæ˜¯å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–ç»“æœ
                logger.error(f"{exchange_names[i]} æŠ“å–å¤±è´¥: {result}")
            else:
                # æ­£å¸¸ç»“æœï¼Œæ·»åŠ åˆ°æ€»åˆ—è¡¨
                logger.info(f"{exchange_names[i]} æŠ“å–æˆåŠŸï¼Œè·å– {len(result)} æ¡æ–°é—»")
                all_news.extend(result)
        
        # è®°å½•æ€»æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"æ‰€æœ‰äº¤æ˜“æ‰€æŠ“å–å®Œæˆï¼Œæ€»è€—æ—¶ {duration:.2f} ç§’ï¼Œå…±è·å– {len(all_news)} æ¡æ–°é—»")
        
        return all_news

    except Exception as e:
        logger.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return []
import os
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import logging
import subprocess
import sys

import shutil
import tempfile

def clean_temp_files():
    """
    æ¸…ç†Playwrightä¸´æ—¶æ–‡ä»¶
    """
    try:
        # æ¸…ç†/tmpç›®å½•ä¸‹çš„playwrightç›¸å…³æ–‡ä»¶
        temp_dir = tempfile.gettempdir()
        for item in os.listdir(temp_dir):
            if item.startswith('playwright') or item.startswith('pyppeteer'):
                item_path = os.path.join(temp_dir, item)
                try:
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {item_path}")
                except Exception as e:
                    logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶è¿‡ç¨‹ä¸­å‡ºé”™: {e}")




logger = logging.getLogger(__name__)

async def main():
    """
    ä¸»å‡½æ•°ï¼šå¹¶è¡ŒæŠ“å–æ‰€æœ‰äº¤æ˜“æ‰€çš„æ–°é—»ï¼Œå¹¶åˆå¹¶ç»“æœ
    
    Returns:
        list: åˆå¹¶åçš„æ–°é—»åˆ—è¡¨
    """
    # æ£€æŸ¥è¿è¡Œç¯å¢ƒ
    is_railway = os.environ.get('RAILWAY_ENVIRONMENT') is not None
    environment_name = "Railwayç¯å¢ƒ" if is_railway else "æœ¬åœ°ç¯å¢ƒ"
    logger.info(f"ğŸŒ å½“å‰åœ¨ã€{environment_name}ã€‘ä¸­æ‰§è¡ŒæŠ“å–ä»»åŠ¡")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()
    
    try:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        all_news = []
        exchange_names = ["Binance", "OKX", "Bitget", "Bybit", "KuCoin", "Gate.io"]
        tasks = [
            fetch_binance_news,
            fetch_okx_news,
            fetch_bitget_news,
            fetch_bybit_news,  # Bybitä½¿ç”¨APIï¼Œä¸å—å½±å“
            fetch_kucoin_news,
            fetch_gate_news
        ]
        
        # ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªæµè§ˆå™¨ä»»åŠ¡
        for i, task_func in enumerate(tasks):
            try:
                result = await task_func()
                logger.info(f"{exchange_names[i]} æŠ“å–æˆåŠŸï¼Œè·å– {len(result)} æ¡æ–°é—»")
                all_news.extend(result)
            except Exception as e:
                logger.error(f"{exchange_names[i]} æŠ“å–å¤±è´¥: {e}")
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(all_news)} æ¡æ–°é—»")
        return all_news
    except Exception as e:
        logger.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return []  # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨


if __name__ == '__main__':
    asyncio.run(main())
